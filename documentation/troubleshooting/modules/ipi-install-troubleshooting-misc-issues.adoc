[id="ipi-install-troubleshooting-misc-issues"]
= Miscellaneous Issues

== runtime network not ready

After cluster deployment if you see
`+runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: Missing CNI default network+`
we need to inspect the pods in `+openshift-network-operator+` namespace.

[source,bash]
----
oc get all -n openshift-network-operator
NAME                                    READY   STATUS              RESTARTS   AGE
pod/network-operator-69dfd7b577-bg89v   0/1     ContainerCreating   0          149m
----

The cluster network operator is responsible for deploying the networking
components. It does this in response to a special object created by the
installer.

From a deployment perspective, the network operator is often the
``canary in the coal mine``. It runs very early in the installation
process, after the master nodes have come up but before the bootstrap
control plane has been torn down. It can be indicative of more subtle
installer issues, such as long delays in bringing up master nodes or
issues with `+apiserver+` communication.

First, determine that the network configuration exists, from
`+provisioner+` node:

[source,bash]
----
[kni@provisioner ~]$ kubectl get network.config.openshift.io cluster -oyaml
apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNetwork:
  - 172.30.0.0/16
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
----

If it doesn’t exist, the installer didn’t create it. You’ll have to run
`+openshift-install create manifests+` to determine why.

Next, check that the network-operator is running, from `+provisioner+`
node issue:

[source,bash]
----
[kni@provisioner ~]$ kubectl -n openshift-network-operator get pods
----

And retrieve the logs. Note that, on multi-master systems, the operator
will perform leader election and all other operators will sleep:

[source,bash]
----
[kni@provisioner ~]$ kubectl -n openshift-network-operator logs -l "name=network-operator"
----

*Note:* For more details please refer
https://github.com/openshift/installer/blob/master/docs/user/troubleshooting.md[Troubleshooting]




== Servers not getting the correct IPv6 over DHCP


. The reserved IPv6 addresses should be outside your DHCP Range
. In the reservation make sure you’re using the correct MAC Address and
Client ID (if supported), example below:
+
[source,bash]
----
# This is a dnsmasq dhcp reservation, 'id:00:03:00:01' is the client id and '18:db:f2:8c:d5:9f' is the MAC Address for the NIC
id:00:03:00:01:18:db:f2:8c:d5:9f,openshift-master-1,[2620:52:0:1302::6]
----
. Make sure Route Announcements are working and that DHCP server is
listening on the required interfaces serving the required ranges

== Servers not getting the correct hostname over DHCP

During the IPv6 deployment you need your servers to get their hostname
over DHCP, we faced several issues when for some reason (we couldn’t
figure out why yet) the hostname is not assigned by NetworkManager right
away.

If you login in the master nodes and you find something like this:

....
Failed Units: 2
  NetworkManager-wait-online.service
  nodeip-configuration.service
....

Your node likely booted up without hostname, which cause Kubelet to boot
with `+localhost.localdomain+` hostname. This will cause several issues,
we’re going to going hrought the different fixes below.


. Force the hostname renewal
+
[source,bash]
----
# Check hostname, if it's localhost run below steps.
hostname
# Wired Connection 5 corresponds to the connection which is assigned to the NIC connected to the baremetal network, it may be different in your env
# This will force the host to renew the dhcp lease
sudo nmcli con up "Wired connection 5"
# Check hostname again
hostname
# If the hostname is still localhost.localdomain restart NetworkManager
sudo systemctl restart NetworkManager
# If the hostname is still localhost.localdomain wait a few seconds/minutes and check again, if same, repeat previous steps
----
. Restart `+nodeip-configuration+` service
+
[source,bash]
----
# This service will reconfigure Kubelet service with the correct hostname references
sudo systemctl restart nodeip-configuration.service
----
. Restart `+kubelet+` service
+
[source,bash]
----
# We need to reload the unit files definition since kubelet one was changed by previous step
sudo systemctl daemon-reload
# Restart kubelet
sudo systemctl restart kubelet.service
----
. Ensure `+kubelet+` booted with correct hostname
+
[source,bash]
----
# Tail the journal and find the hostname
sudo journalctl -fu kubelet.service
----

If you already had this cluster running when the node booted up with the
wrong hostname you will have pending `+csrs+` on the cluster, you *MUST
NOT* approve them, therewise you will face even more issues.

[source,bash]
----
# Get CSR on the cluster
oc get csr
# If you see Pending csr ensure they are good ones
oc get csr <pending_csr> -o jsonpath='{.spec.request}' | base64 -d | openssl req -noout -text
# If you see Subject Name: localhost.localdomain remove the csr
oc delete csr <wrong_csr>
----
