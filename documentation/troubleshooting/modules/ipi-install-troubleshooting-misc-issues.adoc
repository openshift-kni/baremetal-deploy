[id="ipi-install-troubleshooting-misc-issues"]
[[misc]]
= Miscellaneous Issues

== runtime network not ready

After the deployment of a cluster if you receive such an error
`+runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: Missing CNI default network+`
we need to inspect the pods in `+openshift-network-operator+` namespace.

[source,bash]
----
[kni@provisioner ~]$ oc get all -n openshift-network-operator
NAME                                    READY   STATUS              RESTARTS   AGE
pod/network-operator-69dfd7b577-bg89v   0/1     ContainerCreating   0          149m
----

The cluster network operator is responsible for deploying the networking
components. It does this in response to a special object created by the
installer.

It runs very early in the installation
process, after the master nodes have come up but before the bootstrap
control plane has been torn down. It can be indicative of more subtle
installer issues, such as long delays in bringing up master nodes or
issues with `+apiserver+` communication.

First, determine that the network configuration exists, from
`+provisioner+` node:

[source,bash]
----
[kni@provisioner ~]$ kubectl get network.config.openshift.io cluster -oyaml
apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNetwork:
  - 172.30.0.0/16
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
----

If it doesn’t exist, the installer didn’t create it. You’ll have to run
`+openshift-install create manifests+` to determine why.

Next, check that the network-operator is running, from `+provisioner+`
node issue:

[source,bash]
----
[kni@provisioner ~]$ kubectl -n openshift-network-operator get pods
----

and retrieve the logs. Note that, on multi-master systems, the operator
will perform leader election and all other operators will sleep:

[source,bash]
----
[kni@provisioner ~]$ kubectl -n openshift-network-operator logs -l "name=network-operator"
----

*Note:* For more details please refer
https://github.com/openshift/installer/blob/master/docs/user/troubleshooting.md[Troubleshooting]


== Servers not getting the correct IPv6 over DHCP

. The reserved IPv6 addresses should be outside your DHCP Range
. In the reservation make sure you’re using the correct MAC Address and
Client ID (if supported), example below:
+
[source,bash]
----
# This is a dnsmasq dhcp reservation, 'id:00:03:00:01' is the client id and '18:db:f2:8c:d5:9f' is the MAC Address for the NIC
id:00:03:00:01:18:db:f2:8c:d5:9f,openshift-master-1,[2620:52:0:1302::6]
----
. Make sure Route Announcements are working and that DHCP server is
listening on the required interfaces serving the required ranges

== Servers not getting the correct hostname over DHCP

During the IPv6 deployment you need your servers to get their hostname
over DHCP and at times the hostname is not assigned by NetworkManager right
away.

If you login in the master nodes and you find something like this:

....
Failed Units: 2
  NetworkManager-wait-online.service
  nodeip-configuration.service
....

The node likely booted up without hostname, which cause `kubelet` to boot
with `+localhost.localdomain+` hostname. The following steps can be taken to
attempt to remedy.

. Force the hostname renewal
+
[source,bash]
----
# Check hostname, if it's localhost run below steps.
hostname
# Wired Connection 5 corresponds to the connection which is assigned to the NIC connected to the baremetal network, it may be different in your env
# This will force the host to renew the dhcp lease
[core@master-X ~]$ sudo nmcli con up "Wired connection 5"
# Check hostname again
[core@master-X ~]$ hostname
# If the hostname is still localhost.localdomain restart NetworkManager
[core@master-X ~]$ sudo systemctl restart NetworkManager
# If the hostname is still localhost.localdomain wait a few seconds/minutes and check again, if same, repeat previous steps
----
. Restart `+nodeip-configuration+` service
+
[source,bash]
----
# This service will reconfigure Kubelet service with the correct hostname references
[core@master-X ~]$ sudo systemctl restart nodeip-configuration.service
----
. Restart `+kubelet+` service
+
[source,bash]
----
# We need to reload the unit files definition since kubelet one was changed by previous step
sudo systemctl daemon-reload
# Restart kubelet
[core@master-X ~]$ sudo systemctl restart kubelet.service
----
. Ensure `+kubelet+` booted with correct hostname
+
[source,bash]
----
# Tail the journal and find the hostname
[core@master-X ~]$ sudo journalctl -fu kubelet.service
----

NOTE: where `X` is the master node number.

If you already had this cluster running when the node booted up with the
wrong hostname you will have pending `+csrs+` on the cluster, you *MUST
NOT* approve them, therewise you will face even more issues.

[source,bash]
----
# Get CSR on the cluster
[kni@provisioner ~]$ oc get csr
# If you see Pending csr ensure they are good ones
[kni@provisioner ~]$ oc get csr <pending_csr> -o jsonpath='{.spec.request}' | base64 -d | openssl req -noout -text
# If you see Subject Name: localhost.localdomain remove the csr
[kni@provisioner ~]$ oc delete csr <wrong_csr>
----
