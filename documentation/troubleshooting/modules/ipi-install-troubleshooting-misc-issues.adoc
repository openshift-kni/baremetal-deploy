[id="ipi-install-troubleshooting-misc-issues"]
[[misc]]
= Miscellaneous Issues

== runtime network not ready

After the deployment of a cluster if you receive such an error
`+runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: Missing CNI default network+`
we need to inspect the pods in `+openshift-network-operator+` namespace.

[source,bash]
----
[kni@provisioner ~]$ oc get all -n openshift-network-operator
NAME                                    READY   STATUS              RESTARTS   AGE
pod/network-operator-69dfd7b577-bg89v   0/1     ContainerCreating   0          149m
----

The cluster network operator is responsible for deploying the networking
components. It does this in response to a special object created by the
installer.

It runs very early in the installation
process, after the master nodes have come up but before the bootstrap
control plane has been torn down. It can be indicative of more subtle
installer issues, such as long delays in bringing up master nodes or
issues with `+apiserver+` communication.

First, determine that the network configuration exists, from
`+provisioner+` node:

[source,bash]
----
[kni@provisioner ~]$ kubectl get network.config.openshift.io cluster -oyaml
apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNetwork:
  - 172.30.0.0/16
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
----

If it doesn’t exist, the installer didn’t create it. You’ll have to run
`+openshift-install create manifests+` to determine why.

Next, check that the network-operator is running, from `+provisioner+`
node issue:

[source,bash]
----
[kni@provisioner ~]$ kubectl -n openshift-network-operator get pods
----

and retrieve the logs. Note that, on multi-master systems, the operator
will perform leader election and all other operators will sleep:

[source,bash]
----
[kni@provisioner ~]$ kubectl -n openshift-network-operator logs -l "name=network-operator"
----

*Note:* For more details please refer
https://github.com/openshift/installer/blob/master/docs/user/troubleshooting.md[Troubleshooting]


== Servers not getting the correct IPv6 over DHCP

. The reserved IPv6 addresses should be outside your DHCP Range
. In the reservation make sure you’re using the correct DUID, example below:
+
[source,bash]
----
# This is a dnsmasq dhcp reservation, 'id:00:03:00:01' is the client id and '18:db:f2:8c:d5:9f' is the MAC Address for the NIC
id:00:03:00:01:18:db:f2:8c:d5:9f,openshift-master-1,[2620:52:0:1302::6]
----
. Make sure Route Announcements are working and that DHCP server is
listening on the required interfaces serving the required ranges

== Servers not getting the correct hostname over DHCP

During the IPv6 deployment you need your servers to get their hostname
over DHCP and at times the hostname is not assigned by NetworkManager right
away.

If you login in the master nodes and you find something like this:

....
Failed Units: 2
  NetworkManager-wait-online.service
  nodeip-configuration.service
....

The node likely booted up without hostname, which cause `kubelet` to boot
with `+localhost.localdomain+` hostname. The following steps can be taken to
attempt to remedy.

. Force the hostname renewal
+
[source,bash]
----
# Check hostname, if it's localhost run below steps.
hostname
# Wired Connection 5 corresponds to the connection which is assigned to the NIC connected to the baremetal network, it may be different in your env
# This will force the host to renew the dhcp lease
[core@master-X ~]$ sudo nmcli con up "Wired connection 5"
# Check hostname again
[core@master-X ~]$ hostname
# If the hostname is still localhost.localdomain restart NetworkManager
[core@master-X ~]$ sudo systemctl restart NetworkManager
# If the hostname is still localhost.localdomain wait a few seconds/minutes and check again, if same, repeat previous steps
----
. Restart `+nodeip-configuration+` service
+
[source,bash]
----
# This service will reconfigure Kubelet service with the correct hostname references
[core@master-X ~]$ sudo systemctl restart nodeip-configuration.service
----
. Restart `+kubelet+` service
+
[source,bash]
----
# We need to reload the unit files definition since kubelet one was changed by previous step
sudo systemctl daemon-reload
# Restart kubelet
[core@master-X ~]$ sudo systemctl restart kubelet.service
----
. Ensure `+kubelet+` booted with correct hostname
+
[source,bash]
----
# Tail the journal and find the hostname
[core@master-X ~]$ sudo journalctl -fu kubelet.service
----

NOTE: where `X` is the master node number.

If you already had this cluster running when the node booted up with the
wrong hostname you will have pending `+csrs+` on the cluster, you *MUST
NOT* approve them, therewise you will face even more issues.

[source,bash]
----
# Get CSR on the cluster
[kni@provisioner ~]$ oc get csr
# If you see Pending csr ensure they are good ones
[kni@provisioner ~]$ oc get csr <pending_csr> -o jsonpath='{.spec.request}' | base64 -d | openssl req -noout -text
# If you see Subject Name: localhost.localdomain remove the csr
[kni@provisioner ~]$ oc delete csr <wrong_csr>
----

== Routes don't reach endpoints on BM installations 

During the installation process, it is possible to encounter a VRRP conflict if a previously used {product-title} node that was once part of a cluster deployment using a specific cluster name is still running but not part of current {product-title} cluster deployment using that same cluster name.

For example, imagine a scenario were a deployment was done using the cluster name `openshift`, deploying 3 master nodes and 3 worker nodes. Once this install was complete, a future install was started using the same cluster name `openshift` but this redeployment only installed 3 master nodes, leaving the 3 worker nodes from a previous deployment in an `ON` state. This would cause a VRID conflict and VRRP errors.

```sh
[kni@provisioner ~]$ oc get route oauth-openshift 
NAME              HOST/PORT                                  PATH            SERVICES          PORT       TERMINATION            WILDCARD
oauth-openshift   oauth-openshift.apps.example.com                          oauth-openshift     6443      passthrough/Redirect     None
```

Check the service endpoint.

```sh
[kni@provisioner ~]$ oc get svc oauth-openshift 
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
oauth-openshift   ClusterIP   172.30.19.162   <none>        443/TCP   59m
```

Try to reach the service from the master node.

```sh
[core@master0 ~]$ curl -k https://172.30.19.162
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
  },
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {
  },
  "code": 403
```

Also notice the errors from auth operator from provisioner node

```sh
[kni@provisioner ~]$ oc logs deployment/authentication-operator -n openshift-authentication-operator 
Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-authentication-operator", Name:"authentication-operator", UID:"225c5bd5-b368-439b-9155-5fd3c0459d98", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/authentication changed: Degraded message changed from "IngressStateEndpointsDegraded: All 2 endpoints for oauth-server are reporting
```

.Solution

* Ensure that the cluster name for every deployment is unique ensuring no conflict.
* Turn off all the rogue nodes which are not part of the cluster deployment that are using the same cluster name. Otherwise, the authentication pod of your {product-title} cluster may never start successfully.
